image: docker:20.10.16

services:
  - docker:20.10.16-dind

variables:
  DOCKER_HOST: tcp://docker:2375
  DOCKER_TLS_CERTDIR: ""
  DOCKER_DRIVER: overlay2
  DOCKER_REGISTRY: ${CI_REGISTRY}
  DOCKER_IMAGE: ${CI_REGISTRY}/aradhya24/cold-email-generator:${CI_COMMIT_SHA}
  AWS_USER: ubuntu
  LB_DNS: ${LB_DNS}

stages:
  - validate
  - build
  - deploy
  - monitor

validate:
  image: python:3.9-slim
  stage: validate
  script:
    - apt-get update && apt-get install -y python3-pip
    - pip install -r requirements.txt
    - python -c "import app.main" || echo "Validation failed but continuing"
  rules:
    - if: $SKIP_VALIDATE_BUILD == null

build:
  stage: build
  script: 
    - docker login -u gitlab-ci-token -p $CI_JOB_TOKEN $CI_REGISTRY 
    - docker build --pull -t $DOCKER_IMAGE .
    - docker tag $DOCKER_IMAGE $CI_REGISTRY/aradhya24/cold-email-generator:latest
    - docker push $DOCKER_IMAGE 
    - docker push $CI_REGISTRY/aradhya24/cold-email-generator:latest
  rules:
    - if: $SKIP_VALIDATE_BUILD == null

deploy:
  stage: deploy
  image: docker:20.10.16
  services:
    - docker:20.10.16-dind
  variables:
    DOCKER_TLS_CERTDIR: ""
    DOCKER_HOST: tcp://docker:2375
    DOCKER_DRIVER: overlay2
    DOCKER_TLS_VERIFY: ""
    DOCKER_CERT_PATH: ""
  before_script:
    # Install required packages
    - apk add --no-cache openssh-client curl python3 py3-pip git
    
    # Clone the repository to ensure we have all files
    - git config --global user.email "gitlab-ci@example.com"
    - git config --global user.name "GitLab CI"
    - git clone $CI_REPOSITORY_URL ./ || echo "Repository already exists"
    
    # Install AWS CLI
    - pip3 install awscli
    
    # Setup SSH
    - eval $(ssh-agent -s)
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh
    
    # Generate a new SSH key for this pipeline run
    - echo "Generating new SSH key..."
    - ssh-keygen -t rsa -b 2048 -f ~/.ssh/ec2_key -N ""
    - chmod 600 ~/.ssh/ec2_key
    - ssh-add ~/.ssh/ec2_key
    - cat ~/.ssh/ec2_key.pub > ec2_key.pub
    
    # Set up known hosts and disable strict host checking
    - echo -e "Host *\n\tStrictHostKeyChecking no\n\tUserKnownHostsFile=/dev/null\n" > ~/.ssh/config
    
    # Configure AWS credentials and region
    - export AWS_DEFAULT_REGION="us-east-1"
    - aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID
    - aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY
    - aws configure set region us-east-1
    - aws configure set output json
    
    # Debug AWS configuration
    - echo "Verifying AWS configuration:"
    - aws configure list
    - echo "Testing AWS connection:"
    - aws ec2 describe-regions --region us-east-1 --output text | head -1 || echo "AWS connection failed"
  script:
    - |
      # Launch EC2 instance with explicit network interface to ensure public IP
      echo "Launching EC2 instance..."
      
      # Check if required AWS variables are set
      if [ -z "$EC2_SECURITY_GROUP" ]; then
        echo "ERROR: EC2_SECURITY_GROUP is not set. Please add this variable in GitLab CI/CD settings."
        echo "Example value: sg-0123456789abcdef"
        exit 1
      fi
      
      if [ -z "$EC2_SUBNET_ID" ]; then
        echo "ERROR: EC2_SUBNET_ID is not set. Please add this variable in GitLab CI/CD settings."
        echo "Example value: subnet-0123456789abcdef"
        exit 1
      fi
      
      # Launch instance with a simple user data script
      echo "#!/bin/bash" > userdata.sh
      echo "mkdir -p /home/ubuntu/.ssh" >> userdata.sh
      cat ec2_key.pub > pubkey.txt
      echo "cat > /home/ubuntu/.ssh/authorized_keys << 'EOT'" >> userdata.sh
      cat pubkey.txt >> userdata.sh
      echo "EOT" >> userdata.sh
      echo "chmod 700 /home/ubuntu/.ssh" >> userdata.sh
      echo "chmod 600 /home/ubuntu/.ssh/authorized_keys" >> userdata.sh
      echo "chown -R ubuntu:ubuntu /home/ubuntu/.ssh" >> userdata.sh
      
      echo "User data script contents:"
      cat userdata.sh
      
      INSTANCE_ID=$(aws ec2 run-instances \
        --region us-east-1 \
        --image-id ami-0c7217cdde317cfec \
        --instance-type t2.micro \
        --network-interfaces "AssociatePublicIpAddress=true,DeviceIndex=0,Groups=$EC2_SECURITY_GROUP,SubnetId=$EC2_SUBNET_ID" \
        --tag-specifications "ResourceType=instance,Tags=[{Key=Name,Value=cold-email-instance}]" \
        --user-data file://userdata.sh \
        --query 'Instances[0].InstanceId' \
        --output text)
      
      if [ -z "$INSTANCE_ID" ]; then
        echo "Failed to launch EC2 instance."
        exit 1
      fi
      
      echo "Instance ID: $INSTANCE_ID"
      
      # Wait for instance to be running
      echo "Waiting for instance to be running..."
      aws ec2 wait instance-running --region us-east-1 --instance-ids $INSTANCE_ID
      
      # Additional wait for instance to fully initialize
      echo "Instance is running. Waiting 30 seconds for full initialization..."
      sleep 30
      
      # Get public IP with retries and longer wait times
      echo "Getting instance public IP..."
      for i in 1 2 3 4 5; do
        PUBLIC_IP=$(aws ec2 describe-instances \
          --region us-east-1 \
          --instance-ids $INSTANCE_ID \
          --query 'Reservations[0].Instances[0].PublicIpAddress' \
          --output text)
        
        if [ ! -z "$PUBLIC_IP" ] && [ "$PUBLIC_IP" != "None" ] && [ "$PUBLIC_IP" != "null" ]; then
          echo "Got public IP: $PUBLIC_IP"
          break
        fi
        
        echo "Attempt $i: No public IP yet, waiting 20 seconds..."
        sleep 20
      done
      
      if [ -z "$PUBLIC_IP" ] || [ "$PUBLIC_IP" = "None" ] || [ "$PUBLIC_IP" = "null" ]; then
        echo "Failed to get public IP after 5 attempts"
        echo "Terminating instance $INSTANCE_ID to avoid unnecessary charges"
        aws ec2 terminate-instances --region us-east-1 --instance-ids $INSTANCE_ID
        exit 1
      fi
      
      # Create lb_dns.env file
      echo "Creating lb_dns.env file..."
      echo "LB_DNS=$PUBLIC_IP" > lb_dns.env
      cat lb_dns.env
      
      # Wait for SSH to be available
      echo "Waiting for SSH to become available..."
      for i in 1 2 3 4 5 6; do
        if ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ConnectTimeout=10 -i ~/.ssh/ec2_key ubuntu@$PUBLIC_IP "echo 'SSH connection successful'"; then
          echo "SSH connection established"
          break
        fi
        echo "Attempt $i: SSH not ready yet, waiting 20 seconds..."
        sleep 20
        if [ $i -eq 6 ]; then
          echo "Failed to establish SSH connection after 6 attempts"
          echo "Terminating instance $INSTANCE_ID to avoid unnecessary charges"
          aws ec2 terminate-instances --region us-east-1 --instance-ids $INSTANCE_ID
          exit 1
        fi
      done
      
      # Create necessary directories and set permissions
      echo "Creating directories and setting permissions..."
      ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i ~/.ssh/ec2_key ubuntu@$PUBLIC_IP "sudo mkdir -p /opt/cold-email/{app,scripts,k8s} && sudo chown -R ubuntu:ubuntu /opt/cold-email"
      
      # List local directory contents for debugging
      echo "Local directory contents:"
      ls -la
      
      # Copy files in parallel with proper error handling
      echo "Copying files..."
      copy_with_retries() {
        local src="$1"
        local dest="$2"
        local retries=3
        
        # Check if source exists
        if [ ! -e "$src" ]; then
          echo "Source $src does not exist!"
          ls -la $(dirname "$src")
          return 1
        fi
        
        for i in $(seq 1 $retries); do
          if scp -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i ~/.ssh/ec2_key -r "$src" "ubuntu@$PUBLIC_IP:$dest"; then
            return 0
          fi
          echo "Attempt $i: Failed to copy $src, retrying in 5 seconds..."
          sleep 5
        done
        echo "Failed to copy $src after $retries attempts"
        return 1
      }

      # Copy each directory individually
      echo "Copying app directory..."
      copy_with_retries "app" "/opt/cold-email/" || exit 1
      
      echo "Copying scripts directory..."
      copy_with_retries "scripts" "/opt/cold-email/" || exit 1
      
      echo "Copying k8s directory..."
      copy_with_retries "k8s" "/opt/cold-email/" || exit 1
      
      # Verify files were copied
      echo "Verifying file copy..."
      if ! ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i ~/.ssh/ec2_key ubuntu@$PUBLIC_IP "ls -la /opt/cold-email/{app,scripts,k8s}"; then
        echo "Failed to verify copied files"
        exit 1
      fi
      
      # Create and copy configuration files
      echo "Creating configuration files..."
      ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i ~/.ssh/ec2_key ubuntu@$PUBLIC_IP "echo 'GROQ_API_KEY=$GROQ_API_KEY' | sudo tee /opt/cold-email/app/.env && sudo chown ubuntu:ubuntu /opt/cold-email/app/.env"
      
      # Make scripts executable
      echo "Making scripts executable..."
      ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i ~/.ssh/ec2_key ubuntu@$PUBLIC_IP "chmod +x /opt/cold-email/scripts/*.sh || echo 'Failed to make scripts executable'"
      
      # Run setup script
      echo "Running setup script..."
      ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i ~/.ssh/ec2_key ubuntu@$PUBLIC_IP "cd /opt/cold-email && sudo ./scripts/setup-k8s.sh"
      
      # Finalize deployment
      echo "Finalizing deployment..."
      ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i ~/.ssh/ec2_key ubuntu@$PUBLIC_IP "cd /opt/cold-email && sudo ./scripts/deploy-k8s.sh"
      
      echo "Deployment completed successfully!"
  artifacts:
    reports:
      dotenv: lb_dns.env
    expire_in: 1 week
  when: on_success

monitor:
  stage: monitor
  image: ubuntu:22.04
  before_script:
    # Install required packages
    - apt-get update && apt-get install -y openssh-client python3-pip curl
    - pip install awscli
    
    # Setup SSH directory and permissions
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh
    
    # Start SSH agent and ensure it's running
    - eval $(ssh-agent -s)
    - echo "SSH_AUTH_SOCK=$SSH_AUTH_SOCK"
    - echo "SSH_AGENT_PID=$SSH_AGENT_PID"
    
    # Generate and configure SSH key with same key name as deploy stage
    - ssh-keygen -t rsa -b 2048 -f ~/.ssh/ec2_key -N ""
    - chmod 600 ~/.ssh/ec2_key
    - ssh-add ~/.ssh/ec2_key
    - echo -e "Host *\n\tStrictHostKeyChecking no\n\tUserKnownHostsFile=/dev/null\n\tServerAliveInterval 30\n\tServerAliveCountMax 3\n" > ~/.ssh/config
    - chmod 600 ~/.ssh/config
    
    # Configure AWS credentials
    - export AWS_DEFAULT_REGION=us-east-1
    - aws configure set aws_access_key_id $AWS_ACCESS_KEY_ID
    - aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY
    - aws configure set region us-east-1
    - aws configure set output json
  script:
    # Verify AWS configuration
    - aws configure list
    - aws ec2 describe-regions --region us-east-1 --output text | head -1
    
    # Get EC2 instance information with improved error handling
    - |
      MAX_RETRIES=5
      RETRY_DELAY=15
      
      for i in $(seq 1 $MAX_RETRIES); do
        echo "Attempt $i to get EC2 instance info..."
        
        INSTANCE_INFO=$(aws ec2 describe-instances \
          --region us-east-1 \
          --filters "Name=tag:Name,Values=cold-email-instance" "Name=instance-state-name,Values=running" \
          --query "Reservations[*].Instances[*].[InstanceId,PublicIpAddress]" \
          --output text)
        
        EC2_IP=$(echo "$INSTANCE_INFO" | awk '{print $2}')
        
        if [ ! -z "$EC2_IP" ] && [ "$EC2_IP" != "None" ] && [ "$EC2_IP" != "null" ]; then
          echo "Found EC2 instance with IP: $EC2_IP"
          break
        fi
        
        if [ $i -eq $MAX_RETRIES ]; then
          echo "Failed to get EC2 instance IP after $MAX_RETRIES attempts. Instance info:"
          echo "$INSTANCE_INFO"
          exit 1
        fi
        
        echo "No valid IP found, waiting $RETRY_DELAY seconds before retry..."
        sleep $RETRY_DELAY
      done
      
      # Create lb_dns.env file with proper format and permissions
      echo "Creating lb_dns.env file..."
      echo "LB_DNS=\"$EC2_IP\"" > lb_dns.env
      chmod 600 lb_dns.env
      
      # Copy SSH key from deploy stage (if available)
      if [ -f "/builds/aradhya24/cold-email-generator/ec2_key.pub" ]; then
        echo "Found existing public key, using it..."
        cat "/builds/aradhya24/cold-email-generator/ec2_key.pub" >> ~/.ssh/authorized_keys
      fi
      
      # Function to run SSH commands with retries and debug output
      run_ssh_command() {
        local command="$1"
        local max_attempts=3
        local wait_time=10
        
        # Debug SSH configuration
        echo "SSH Debug: Checking key permissions and content"
        ls -la ~/.ssh/
        ssh-add -l || echo "No keys in agent"
        
        for attempt in $(seq 1 $max_attempts); do
          echo "Attempt $attempt: Running command via SSH..."
          if ssh -v -i ~/.ssh/ec2_key -o ConnectTimeout=10 ubuntu@${EC2_IP} "$command" 2>&1; then
            echo "Command executed successfully!"
            return 0
          fi
          if [ $attempt -lt $max_attempts ]; then
            echo "Command failed. Waiting $wait_time seconds before retry..."
            sleep $wait_time
          fi
        done
        echo "Failed to execute command after $max_attempts attempts."
        return 1
      }
      
      # Test SSH connection first
      echo "Testing SSH connection..."
      run_ssh_command "echo 'SSH test successful'" || exit 1
      
      # Check Kubernetes cluster status
      echo "Checking Kubernetes cluster status..."
      run_ssh_command "sudo kubectl get nodes --no-headers" || exit 1
      run_ssh_command "sudo kubectl get pods --all-namespaces" || exit 1
      
      # Verify application deployment
      echo "Verifying application deployment..."
      run_ssh_command "sudo kubectl get pods -n default -l app=cold-email-generator -o wide" || exit 1
      
      # Check service status
      echo "Checking service status..."
      run_ssh_command "sudo kubectl get svc -n default" || exit 1
      
      echo "Monitor stage completed successfully!"
  artifacts:
    reports:
      dotenv: lb_dns.env
    paths:
      - lb_dns.env
    expire_in: 1 week
  rules:
    - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH