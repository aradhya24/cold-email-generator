image: docker:20.10.16

services:
  - docker:20.10.16-dind

variables:
  DOCKER_HOST: tcp://docker:2375
  DOCKER_TLS_CERTDIR: ""
  DOCKER_DRIVER: overlay2
  DOCKER_REGISTRY: ${CI_REGISTRY}
  DOCKER_IMAGE: ${CI_REGISTRY}/aradhya24/cold-email-generator:${CI_COMMIT_SHA}
  AWS_USER: ubuntu
  LB_DNS: ${LB_DNS}

stages:
  - validate
  - build
  - deploy_setup
  - deploy
  - monitor

validate:
  image: python:3.9-slim
  stage: validate
  script:
    - apt-get update && apt-get install -y python3-pip
    - pip install -r requirements.txt
    - echo "Validating project structure..."
    - python -c "import app.main" || echo "Validation failed but continuing"

build:
  stage: build
  script: 
    - docker login -u gitlab-ci-token -p $CI_JOB_TOKEN $CI_REGISTRY 
    - docker build --pull -t $DOCKER_IMAGE .
    - docker tag $DOCKER_IMAGE $CI_REGISTRY/aradhya24/cold-email-generator:latest
    - docker push $DOCKER_IMAGE 
    - docker push $CI_REGISTRY/aradhya24/cold-email-generator:latest

deploy_setup:
  stage: deploy_setup
  image: python:3.9-slim
  script:
    - apt-get update && apt-get install -y openssh-client gettext-base awscli jq
    - mkdir -p ~/.ssh && chmod 700 ~/.ssh
    - echo "$AWS_SSH_KEY" | tr -d '\r' > ~/.ssh/id_rsa
    - chmod 600 ~/.ssh/id_rsa
    - eval $(ssh-agent -s) && ssh-add ~/.ssh/id_rsa
    - echo -e "Host *\n\tStrictHostKeyChecking no\n\n" > ~/.ssh/config
    - export AWS_DEFAULT_REGION=us-east-1
    - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
    - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
    
    # Get EC2 instance info
    - chmod +x scripts/get_healthy_instance.sh
    - EC2_IP=$(./scripts/get_healthy_instance.sh)
    - echo "Using EC2 instance with IP $EC2_IP for deployment"
    
    # Get detailed instance info
    - echo "Getting detailed instance information..."
    - INSTANCE_ID=$(aws ec2 describe-instances --filters "Name=ip-address,Values=$EC2_IP" --query "Reservations[0].Instances[0].InstanceId" --output text || echo "")
    - if [ -z "$INSTANCE_ID" ]; then
    -   echo "Could not find instance with IP $EC2_IP, trying public-ip-address filter..."
    -   INSTANCE_ID=$(aws ec2 describe-instances --filters "Name=public-ip-address,Values=$EC2_IP" --query "Reservations[0].Instances[0].InstanceId" --output text || echo "")
    - fi
    - if [ -z "$INSTANCE_ID" ]; then
    -   echo "ERROR: Could not find EC2 instance with IP $EC2_IP"
    -   # List all running instances for debugging
    -   echo "Listing all running instances:"
    -   aws ec2 describe-instances --filters "Name=instance-state-name,Values=running" --query "Reservations[*].Instances[*].{InstanceId:InstanceId,PublicIpAddress:PublicIpAddress,PrivateIpAddress:PrivateIpAddress,LaunchTime:LaunchTime,KeyName:KeyName}" --output table
    -   exit 1
    - fi
    
    - echo "Found instance $INSTANCE_ID with IP $EC2_IP"
    - INSTANCE_STATE=$(aws ec2 describe-instances --instance-ids $INSTANCE_ID --query "Reservations[0].Instances[0].State.Name" --output text)
    - LAUNCH_TIME=$(aws ec2 describe-instances --instance-ids $INSTANCE_ID --query "Reservations[0].Instances[0].LaunchTime" --output text)
    - KEY_NAME=$(aws ec2 describe-instances --instance-ids $INSTANCE_ID --query "Reservations[0].Instances[0].KeyName" --output text)
    - echo "Instance state: $INSTANCE_STATE"
    - echo "Launch time: $LAUNCH_TIME"
    - echo "Key name: $KEY_NAME"
    
    # Wait for instance status checks to pass
    - echo "Waiting for instance status checks to pass..."
    - TIMEOUT=300  # 5 minutes timeout
    - end_time=$(($(date +%s) + TIMEOUT))
    - while [ $(date +%s) -lt $end_time ]; do
    -   STATUS=$(aws ec2 describe-instance-status --instance-ids $INSTANCE_ID --query "InstanceStatuses[0].InstanceStatus.Status" --output text)
    -   SYSTEM_STATUS=$(aws ec2 describe-instance-status --instance-ids $INSTANCE_ID --query "InstanceStatuses[0].SystemStatus.Status" --output text)
    -   echo "Instance status: $STATUS, System status: $SYSTEM_STATUS"
    -   if [ "$STATUS" = "ok" ] && [ "$SYSTEM_STATUS" = "ok" ]; then
    -     echo "Instance is fully initialized and ready!"
    -     break
    -   fi
    -   echo "Instance not fully initialized yet, waiting 10 seconds..."
    -   sleep 10
    - done
    
    # Check and fix security group settings
    - echo "Checking security group for SSH access..."
    - SECURITY_GROUP_ID=$(aws ec2 describe-instances --instance-ids $INSTANCE_ID --query "Reservations[0].Instances[0].SecurityGroups[0].GroupId" --output text)
    - echo "Instance $INSTANCE_ID is using security group $SECURITY_GROUP_ID"
    - echo "Security group rules:"
    - aws ec2 describe-security-groups --group-ids $SECURITY_GROUP_ID --query "SecurityGroups[0].IpPermissions" --output json
    - SSH_RULE_EXISTS=$(aws ec2 describe-security-groups --group-ids $SECURITY_GROUP_ID --query "length(SecurityGroups[0].IpPermissions[?FromPort==\`22\` && ToPort==\`22\` && IpProtocol==\`tcp\`])" --output text)
    - if [ "$SSH_RULE_EXISTS" = "0" ]; then
    -   echo "Adding SSH rule to security group $SECURITY_GROUP_ID"
    -   aws ec2 authorize-security-group-ingress --group-id $SECURITY_GROUP_ID --protocol tcp --port 22 --cidr 0.0.0.0/0
    -   echo "Waiting for security group changes to propagate..."
    -   sleep 15
    - else
    -   echo "SSH rule already exists in security group"
    - fi
    
    # Test SSH with retries and verbose logging
    - echo "Testing SSH connection with retries..."
    - echo "Printing public key being used:"
    - ssh-keygen -y -f ~/.ssh/id_rsa
    
    - for i in {1..5}; do
    -   echo "SSH connection attempt $i..."
    -   if ssh -v -o ConnectTimeout=20 ${AWS_USER}@${EC2_IP} "echo SSH connection successful"; then
    -     echo "SSH connection established on attempt $i!"
    -     break
    -   else
    -     echo "SSH connection failed. Waiting before retry..."
    -     if [ $i -eq 3 ]; then
    -       echo "Three failed attempts. Getting instance console output for debugging:"
    -       aws ec2 get-console-output --instance-id $INSTANCE_ID || echo "Console output not available"
    -       echo "Checking network connectivity with ping:"
    -       ping -c 3 $EC2_IP || echo "Ping failed"
    -     fi
    -     sleep 30
    -   fi
    -   if [ $i -eq 5 ]; then
    -     echo "Could not establish SSH after 5 attempts. Check these common issues:"
    -     echo "1. Security group rules - ensure port 22 is open"
    -     echo "2. AWS_SSH_KEY variable - ensure it matches the key used to launch instance ($KEY_NAME)"
    -     echo "3. Instance status - ensure instance is fully initialized and running"
    -     echo "4. Network ACLs and routing - ensure network allows SSH traffic"
    -     exit 1
    -   fi
    - done
    
    # Once SSH is working, continue with regular setup
    - ssh ${AWS_USER}@${EC2_IP} "mkdir -p ~/k8s ~/scripts"
    - scp -r k8s/* ${AWS_USER}@${EC2_IP}:~/k8s/
    - scp scripts/setup-k8s.sh ${AWS_USER}@${EC2_IP}:~/scripts/
    - ssh ${AWS_USER}@${EC2_IP} "chmod +x ~/scripts/setup-k8s.sh"
    - ssh ${AWS_USER}@${EC2_IP} "export GROQ_API_KEY=${GROQ_API_KEY} && ~/scripts/setup-k8s.sh"
  rules:
    - if: $CI_COMMIT_BRANCH == "main"

deploy:
  stage: deploy
  image: python:3.9-slim
  script: |
    # Install required packages
    apt-get update && apt-get install -y curl unzip openssh-client gettext-base jq
    curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
    unzip awscliv2.zip
    ./aws/install

    # Setup SSH
    mkdir -p ~/.ssh && chmod 700 ~/.ssh
    echo "$AWS_SSH_KEY" | tr -d '\r' > ~/.ssh/id_rsa
    chmod 600 ~/.ssh/id_rsa
    eval $(ssh-agent -s) && ssh-add ~/.ssh/id_rsa
    echo -e "Host *\n\tStrictHostKeyChecking no\n\n" > ~/.ssh/config
    chmod 600 ~/.ssh/config
    echo "### SSH public key being used ###"
    ssh-keygen -y -f ~/.ssh/id_rsa
    echo "###################################################################"

    # Setup AWS credentials
    export AWS_DEFAULT_REGION=us-east-1
    export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
    export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY

    # Find a suitable EC2 instance
    echo "Finding EC2 instance with tag Name=cold-email-instance"
    INSTANCES_JSON=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=cold-email-instance" "Name=instance-state-name,Values=running" --query "Reservations[*].Instances[*].{InstanceId:InstanceId,PublicIpAddress:PublicIpAddress,LaunchTime:LaunchTime}" --output json)
    INSTANCE_COUNT=$(echo $INSTANCES_JSON | jq '.[0] | length')
    echo "Found $INSTANCE_COUNT running instances"

    if [ "$INSTANCE_COUNT" -eq "0" ]; then
      echo "No instances found. Launching a new one..."
      chmod +x scripts/launch_ec2.sh
      ./scripts/launch_ec2.sh
      sleep 30
      EC2_IP=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=cold-email-instance" "Name=instance-state-name,Values=running" --query "Reservations[0].Instances[0].PublicIpAddress" --output text)
    else
      echo "Selecting the most recently launched instance"
      EC2_IP=$(echo $INSTANCES_JSON | jq -r '.[0] | sort_by(.LaunchTime) | reverse | .[0].PublicIpAddress')
    fi

    echo "Deploying to instance $EC2_IP"
    echo "Waiting for SSH to become available..."

    for i in {1..10}; do
      echo "Attempt $i: Testing SSH connection..."
      if ssh -v -o ConnectTimeout=10 ${AWS_USER}@${EC2_IP} "echo SSH connection successful"; then
        echo "SSH connection established!"
        break
      else
        echo "SSH connection failed. Waiting 30 seconds before retry..."
        if [ $i -eq 3 ]; then
          echo "Three failed attempts. Printing instance details for debugging:"
          INSTANCE_ID=$(aws ec2 describe-instances --filters "Name=ip-address,Values=$EC2_IP" --query "Reservations[0].Instances[0].InstanceId" --output text)
          aws ec2 describe-instances --instance-ids $INSTANCE_ID
          echo "This EC2 instance was likely launched with a different key pair."
        fi
        sleep 30
      fi
      
      if [ $i -eq 10 ]; then
        echo "Could not establish SSH connection after 10 attempts. Exiting."
        exit 1
      fi
    done

    # Setup directories and files
    echo "Creating directories on the instance..."
    ssh ${AWS_USER}@${EC2_IP} "mkdir -p ~/k8s ~/scripts"
    
    echo "Copying deployment scripts to the instance..."
    scp -v scripts/setup-k8s.sh ${AWS_USER}@${EC2_IP}:~/scripts/
    scp -v scripts/deploy-k8s.sh ${AWS_USER}@${EC2_IP}:~/scripts/
    scp -v scripts/deploy.sh ${AWS_USER}@${EC2_IP}:~/scripts/
    
    echo "Copying Kubernetes config files..."
    scp -v -r k8s/* ${AWS_USER}@${EC2_IP}:~/k8s/
    
    echo "Making scripts executable..."
    ssh ${AWS_USER}@${EC2_IP} "chmod +x ~/scripts/*.sh"
    
    echo "Setting up the EC2 instance with required dependencies..."
    ssh ${AWS_USER}@${EC2_IP} "sudo apt-get update && sudo apt-get install -y docker.io nginx docker-compose"
    ssh ${AWS_USER}@${EC2_IP} "sudo systemctl enable docker && sudo systemctl start docker"
    
    echo "Creating simple Kubernetes namespace directory structure..."
    ssh ${AWS_USER}@${EC2_IP} "mkdir -p ~/k8s/namespaces/cold-email"
    
    echo "Preparing deployment files..."
    envsubst < k8s/deployment.yaml > k8s/deployment.tmp.yaml
    
    echo "Copying deployment files..."
    scp -v k8s/deployment.tmp.yaml ${AWS_USER}@${EC2_IP}:~/k8s/deployment.yaml

    # Create configuration files
    echo "Creating docker-compose configuration..."
    echo "version: '3'" > docker-compose.yml
    echo "services:" >> docker-compose.yml
    echo "  app:" >> docker-compose.yml
    echo "    image: ${CI_REGISTRY}/aradhya24/cold-email-generator:${CI_COMMIT_SHA}" >> docker-compose.yml
    echo "    ports:" >> docker-compose.yml
    echo "      - \"8501:8501\"" >> docker-compose.yml
    echo "    environment:" >> docker-compose.yml
    echo "      - GROQ_API_KEY=${GROQ_API_KEY}" >> docker-compose.yml
    echo "  " >> docker-compose.yml
    echo "  nginx:" >> docker-compose.yml
    echo "    image: nginx" >> docker-compose.yml
    echo "    ports:" >> docker-compose.yml
    echo "      - \"80:80\"" >> docker-compose.yml
    echo "    volumes:" >> docker-compose.yml
    echo "      - ./nginx.conf:/etc/nginx/conf.d/default.conf" >> docker-compose.yml
    echo "    depends_on:" >> docker-compose.yml
    echo "      - app" >> docker-compose.yml
    
    echo "Creating nginx configuration..."
    echo "server {" > nginx.conf
    echo "  listen 80;" >> nginx.conf
    echo "  server_name _;" >> nginx.conf
    echo "  " >> nginx.conf
    echo "  location / {" >> nginx.conf
    echo "    proxy_pass http://app:8501;" >> nginx.conf
    echo "    proxy_http_version 1.1;" >> nginx.conf
    echo "    proxy_set_header Upgrade \$http_upgrade;" >> nginx.conf
    echo "    proxy_set_header Connection \"upgrade\";" >> nginx.conf
    echo "    proxy_set_header Host \$host;" >> nginx.conf
    echo "  }" >> nginx.conf
    echo "}" >> nginx.conf

    # Copy configuration files to the instance
    echo "Copying the docker-compose and nginx files..."
    scp -v docker-compose.yml ${AWS_USER}@${EC2_IP}:~/docker-compose.yml
    scp -v nginx.conf ${AWS_USER}@${EC2_IP}:~/nginx.conf
    
    echo "Stopping any existing nginx system service..."
    ssh ${AWS_USER}@${EC2_IP} "sudo systemctl stop nginx || true"
    
    # Run the application
    echo "Running the application with Docker Compose..."
    ssh ${AWS_USER}@${EC2_IP} "sudo docker login -u gitlab-ci-token -p ${CI_JOB_TOKEN} ${CI_REGISTRY}"
    ssh ${AWS_USER}@${EC2_IP} "cd ~/ && export CI_REGISTRY=${CI_REGISTRY} && export CI_COMMIT_SHA=${CI_COMMIT_SHA} && export GROQ_API_KEY=${GROQ_API_KEY} && sudo -E docker-compose up -d"
    
    # Finalize
    PUBLIC_IP=$EC2_IP
    
    echo "==========================================================="
    echo "Application deployed successfully and is accessible at:"
    echo "http://${PUBLIC_IP}"
    echo "==========================================================="
    
    echo "LB_DNS=${PUBLIC_IP}" > lb_dns.env
    
    echo "Deployment process completed successfully!"
  environment:
    name: production
    url: http://${LB_DNS}
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - when: never
  artifacts:
    reports:
      dotenv: lb_dns.env

monitor:
  stage: monitor
  image: python:3.9-slim
  script:
    - apt-get update && apt-get install -y openssh-client curl awscli
    - mkdir -p ~/.ssh && chmod 700 ~/.ssh
    - echo "$AWS_SSH_KEY" | tr -d '\r' > ~/.ssh/id_rsa
    - chmod 600 ~/.ssh/id_rsa
    - eval $(ssh-agent -s) && ssh-add ~/.ssh/id_rsa
    - echo -e "Host *\n\tStrictHostKeyChecking no\n\n" > ~/.ssh/config
    - export AWS_DEFAULT_REGION=us-east-1
    - export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
    - export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
    - chmod +x scripts/get_healthy_instance.sh
    - EC2_IP=$(./scripts/get_healthy_instance.sh)
    - ssh ${AWS_USER}@${EC2_IP} "kubectl get pods -n cold-email"
    - ssh ${AWS_USER}@${EC2_IP} "kubectl get svc -n cold-email"
    - |
      if [ -z "$LB_DNS" ]; then
        export LB_DNS=$(aws elbv2 describe-load-balancers \
          --names cold-email-lb \
          --query 'LoadBalancers[0].DNSName' \
          --output text)
      fi
    - echo "Waiting for application to be available at load balancer..."
    - sleep 60
    - echo "Checking application health at http://${LB_DNS}/_stcore/health"
    - curl -s -f -m 10 "http://${LB_DNS}/_stcore/health" || echo "Health check failed, application may need more time to become available"
  rules:
    - if: $CI_COMMIT_BRANCH == "main"