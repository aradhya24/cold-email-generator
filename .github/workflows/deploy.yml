name: Deploy Cold Email Generator

on:
  push:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      setup_infrastructure:
        description: 'Set up AWS infrastructure'
        required: false
        default: true
        type: boolean

env:
  DOCKER_REGISTRY: ghcr.io
  DOCKER_IMAGE: ghcr.io/${{ github.repository_owner }}/cold-email:${{ github.sha }}
  DOCKER_IMAGE_LATEST: ghcr.io/${{ github.repository_owner }}/cold-email:latest
  AWS_USER: ubuntu
  APP_NAME: cold-email
  AWS_REGION: ${{ secrets.AWS_REGION }}

jobs:
  build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      
      - name: Login to GitHub Container Registry
        uses: docker/login-action@v2
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Build and push Docker image
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: |
            ${{ env.DOCKER_IMAGE }}
            ${{ env.DOCKER_IMAGE_LATEST }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  deploy:
    name: Deploy Application
    needs: build
    runs-on: ubuntu-latest
    environment: production
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}
      
      - name: Set up SSH
        run: |
          mkdir -p ~/.ssh
          echo "Setting up SSH key..."
          # Save key to file
          echo "${{ secrets.AWS_SSH_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          
          # Display key fingerprint (for verification)
          ssh-keygen -l -f ~/.ssh/id_rsa || echo "Unable to show key fingerprint"
          
          # Add GitHub and AWS hosts to known hosts
          ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts
          
          # Create a more permissive SSH config for this connection
          cat > ~/.ssh/config << 'EOL'
          Host *
            StrictHostKeyChecking no
            UserKnownHostsFile=/dev/null
            ConnectTimeout 30
            ConnectionAttempts 3
            ServerAliveInterval 30
            ServerAliveCountMax 5
            LogLevel DEBUG3
          EOL
          
          # Show the SSH config
          cat ~/.ssh/config
      
      - name: Set up infrastructure if needed or requested
        run: |
          # Check if infrastructure setup is requested
          if [[ "${{ github.event.inputs.setup_infrastructure }}" == "true" || "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "Setting up infrastructure..."
            chmod +x deploy/aws-k8s/aws-infrastructure.sh
            cd deploy/aws-k8s
            ./aws-infrastructure.sh
            cd ../..
            
            # Verify infrastructure was created
            echo "Verifying infrastructure was created properly..."
            
            # Check if ASG exists
            ASG_EXISTS=$(aws autoscaling describe-auto-scaling-groups \
              --auto-scaling-group-names ${APP_NAME}-asg \
              --query "length(AutoScalingGroups)" \
              --output text || echo "0")
            
            if [ "$ASG_EXISTS" == "0" ]; then
              echo "ERROR: Auto Scaling Group was not created properly!"
              exit 1
            fi
            
            echo "Auto Scaling Group created successfully."
            
            # Check for launch template
            LT_EXISTS=$(aws ec2 describe-launch-templates \
              --launch-template-names ${APP_NAME}-launch-template \
              --query "length(LaunchTemplates)" \
              --output text || echo "0")
            
            if [ "$LT_EXISTS" == "0" ]; then
              echo "ERROR: Launch Template was not created properly!"
              exit 1
            fi
            
            echo "Launch Template created successfully."
            
            # Wait for instances to launch and be fully ready
            echo "Waiting for instances to be ready (5 minutes)..."
            sleep 300
            
            # Check if any instances are running
            RUNNING_INSTANCES=$(aws ec2 describe-instances \
              --filters "Name=tag:aws:autoscaling:groupName,Values=${APP_NAME}-asg" "Name=instance-state-name,Values=running" \
              --query "length(Reservations[].Instances[])" \
              --output text || echo "0")
            
            if [ "$RUNNING_INSTANCES" == "0" ]; then
              echo "WARNING: No running instances found after 5 minutes."
              echo "Waiting an additional 3 minutes..."
              sleep 180
              
              # Check again
              RUNNING_INSTANCES=$(aws ec2 describe-instances \
                --filters "Name=tag:aws:autoscaling:groupName,Values=${APP_NAME}-asg" "Name=instance-state-name,Values=running" \
                --query "length(Reservations[].Instances[])" \
                --output text || echo "0")
              
              if [ "$RUNNING_INSTANCES" == "0" ]; then
                echo "ERROR: No instances are running. Checking ASG activities..."
                
                # Check ASG activities for errors
                aws autoscaling describe-scaling-activities \
                  --auto-scaling-group-name ${APP_NAME}-asg \
                  --max-items 5
                
                echo "Continuing with deployment, but instance setup may have issues."
              fi
            else
              echo "Found $RUNNING_INSTANCES running instances."
            fi
          else
            # Check if ASG exists
            ASG_EXISTS=$(aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names ${APP_NAME}-asg --query "length(AutoScalingGroups)" --output text || echo "0")
            if [ "$ASG_EXISTS" == "0" ]; then
              echo "Infrastructure does not exist yet. Setting it up..."
              chmod +x deploy/aws-k8s/aws-infrastructure.sh
              cd deploy/aws-k8s
              ./aws-infrastructure.sh
              cd ../..
              
              # Verify infrastructure was created
              echo "Verifying infrastructure was created properly..."
              
              # Check if ASG exists now
              ASG_EXISTS=$(aws autoscaling describe-auto-scaling-groups \
                --auto-scaling-group-names ${APP_NAME}-asg \
                --query "length(AutoScalingGroups)" \
                --output text || echo "0")
              
              if [ "$ASG_EXISTS" == "0" ]; then
                echo "ERROR: Auto Scaling Group was not created properly!"
                exit 1
              fi
              
              echo "Auto Scaling Group created successfully."
              echo "Waiting for instances to be ready (5 minutes)..."
              sleep 300
            else
              echo "Infrastructure already exists, checking instance health..."
              
              # Check if any instances are running
              RUNNING_INSTANCES=$(aws ec2 describe-instances \
                --filters "Name=tag:aws:autoscaling:groupName,Values=${APP_NAME}-asg" "Name=instance-state-name,Values=running" \
                --query "length(Reservations[].Instances[])" \
                --output text || echo "0")
              
              if [ "$RUNNING_INSTANCES" == "0" ]; then
                echo "WARNING: No running instances found in existing ASG."
                echo "Checking ASG desired capacity..."
                
                DESIRED_CAPACITY=$(aws autoscaling describe-auto-scaling-groups \
                  --auto-scaling-group-names ${APP_NAME}-asg \
                  --query "AutoScalingGroups[0].DesiredCapacity" \
                  --output text)
                
                if [ "$DESIRED_CAPACITY" == "0" ]; then
                  echo "Desired capacity is 0. Increasing to 1..."
                  aws autoscaling set-desired-capacity \
                    --auto-scaling-group-name ${APP_NAME}-asg \
                    --desired-capacity 1
                  
                  echo "Waiting for instances to launch (3 minutes)..."
                  sleep 180
                else
                  echo "Desired capacity is $DESIRED_CAPACITY, but no running instances found."
                  echo "This may indicate issues with instance launches."
                  echo "ASG activities:"
                  
                  aws autoscaling describe-scaling-activities \
                    --auto-scaling-group-name ${APP_NAME}-asg \
                    --max-items 5
                fi
              else
                echo "Found $RUNNING_INSTANCES running instances."
              fi
            fi
      
      - name: Verify EC2 security group
        run: |
          echo "Verifying security groups for SSH access..."
          
          # Find instances in our ASG
          INSTANCE_IDS=$(aws autoscaling describe-auto-scaling-groups \
            --auto-scaling-group-names ${APP_NAME}-asg \
            --query "AutoScalingGroups[0].Instances[*].InstanceId" \
            --output text)
          
          if [ -z "$INSTANCE_IDS" ]; then
            echo "No instances found in the auto scaling group."
            exit 1
          fi
          
          # Get first instance ID
          INSTANCE_ID=$(echo $INSTANCE_IDS | cut -d' ' -f1)
          echo "Checking security group for instance $INSTANCE_ID"
          
          # Get security group ID for the instance
          SG_ID=$(aws ec2 describe-instances \
            --instance-ids $INSTANCE_ID \
            --query "Reservations[0].Instances[0].SecurityGroups[0].GroupId" \
            --output text)
          
          echo "Security group: $SG_ID"
          
          # Force SSH rule recreation by removing any existing SSH rules
          echo "Ensuring SSH access by recreating SSH rule..."
          
          # List current rules for SSH
          echo "Current SSH rules:"
          aws ec2 describe-security-groups \
            --group-ids $SG_ID \
            --filters "Name=ip-permission.from-port,Values=22" "Name=ip-permission.to-port,Values=22" \
            --query "SecurityGroups[0].IpPermissions" \
            --output json || echo "No SSH rules found"
          
          # Remove any existing SSH rules
          echo "Removing any existing SSH rules..."
          aws ec2 revoke-security-group-ingress \
            --group-id $SG_ID \
            --protocol tcp \
            --port 22 \
            --cidr 0.0.0.0/0 \
            || echo "No SSH rules to remove or removal failed"
          
          # Add fresh SSH rule
          echo "Adding new SSH access rule..."
          aws ec2 authorize-security-group-ingress \
            --group-id $SG_ID \
            --protocol tcp \
            --port 22 \
            --cidr 0.0.0.0/0
          
          echo "SSH access rule added to security group."
          
          # Verify rule was added
          echo "Verifying new SSH rule:"
          aws ec2 describe-security-groups \
            --group-ids $SG_ID \
            --filters "Name=ip-permission.from-port,Values=22" "Name=ip-permission.to-port,Values=22" \
            --query "SecurityGroups[0].IpPermissions" \
            --output json
          
          echo "Waiting 30 seconds for rule to propagate..."
          sleep 30
      
      - name: Find healthy EC2 instance with retries
        run: |
          chmod +x deploy/aws-k8s/get-healthy-instance.sh
          
          # Try multiple times to find a healthy instance
          MAX_RETRIES=10
          RETRY_SLEEP=30
          
          # Debug SSH configuration
          echo "Current directory: $(pwd)"
          echo "SSH Key permissions: $(ls -la ~/.ssh/id_rsa)"
          echo "SSH Config: $(cat ~/.ssh/config)"
          
          for i in $(seq 1 $MAX_RETRIES); do
            echo "Attempt $i of $MAX_RETRIES to find a healthy instance..."
            
            # Get all running instances from ASG
            INSTANCE_IDS=$(aws autoscaling describe-auto-scaling-groups \
              --auto-scaling-group-names ${APP_NAME}-asg \
              --query "AutoScalingGroups[0].Instances[*].InstanceId" \
              --output text)
              
            if [ -z "$INSTANCE_IDS" ]; then
              echo "No instances found in auto scaling group. Waiting 30 seconds..."
              sleep 30
              continue
            fi
            
            echo "Found instances: $INSTANCE_IDS"
            
            # Try to directly connect to each instance
            for INSTANCE_ID in $INSTANCE_IDS; do
              # Get instance information
              echo "Checking instance $INSTANCE_ID..."
              INSTANCE_INFO=$(aws ec2 describe-instances --instance-ids $INSTANCE_ID)
              INSTANCE_STATE=$(echo "$INSTANCE_INFO" | jq -r '.Reservations[0].Instances[0].State.Name')
              
              if [ "$INSTANCE_STATE" != "running" ]; then
                echo "Instance $INSTANCE_ID is not running (state: $INSTANCE_STATE). Skipping..."
                continue
              fi
              
              # Get public IP
              IP=$(echo "$INSTANCE_INFO" | jq -r '.Reservations[0].Instances[0].PublicIpAddress')
              
              if [ -z "$IP" ] || [ "$IP" == "null" ]; then
                echo "No public IP found for instance $INSTANCE_ID."
                continue
              fi
              
              echo "Found instance $INSTANCE_ID with IP $IP"
              
              # Display security group info
              SG_ID=$(echo "$INSTANCE_INFO" | jq -r '.Reservations[0].Instances[0].SecurityGroups[0].GroupId')
              echo "Security group: $SG_ID"
              aws ec2 describe-security-groups --group-ids $SG_ID
              
              # Try pinging the instance first
              echo "Pinging $IP..."
              if ping -c 3 -W 5 $IP; then
                echo "Ping successful, trying SSH..."
              else
                echo "Ping failed for $IP, network may be unreachable"
              fi
              
              # Try a direct SSH connection with extensive debugging
              echo "Attempting SSH connection to $IP with debug mode..."
              
              # Verify we can actually connect to the instance
              if ssh -v -o ConnectTimeout=30 -o StrictHostKeyChecking=no ${AWS_USER}@${IP} "echo SSH connection successful"; then
                echo "SSH connection to $IP verified!"
                echo "EC2_IP=$IP" >> $GITHUB_ENV
                echo "INSTANCE_ID=$INSTANCE_ID" >> $GITHUB_ENV
                break 2  # Break out of both loops
              else
                echo "SSH connection to $IP failed, continuing to next instance or retry..."
              fi
            done
            
            if [ $i -eq $MAX_RETRIES ]; then
              echo "Failed to find a healthy instance after $MAX_RETRIES attempts"
              
              # Display debugging information
              echo "Showing ASG status:"
              aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names ${APP_NAME}-asg
              
              echo "Showing instance details:"
              aws ec2 describe-instances --filters "Name=tag:aws:autoscaling:groupName,Values=${APP_NAME}-asg"
              
              # As a last resort, use any running instance
              echo "As a last resort, using any running instance..."
              RUNNING_INSTANCE=$(aws ec2 describe-instances \
                --filters "Name=tag:aws:autoscaling:groupName,Values=${APP_NAME}-asg" "Name=instance-state-name,Values=running" \
                --query "Reservations[0].Instances[0].InstanceId" \
                --output text)
              
              if [ ! -z "$RUNNING_INSTANCE" ] && [ "$RUNNING_INSTANCE" != "None" ]; then
                IP=$(aws ec2 describe-instances \
                  --instance-ids $RUNNING_INSTANCE \
                  --query "Reservations[0].Instances[0].PublicIpAddress" \
                  --output text)
                
                if [ ! -z "$IP" ] && [ "$IP" != "None" ]; then
                  echo "FALLBACK: Using instance $RUNNING_INSTANCE with IP $IP without SSH validation"
                  echo "EC2_IP=$IP" >> $GITHUB_ENV
                  echo "INSTANCE_ID=$RUNNING_INSTANCE" >> $GITHUB_ENV
                  break
                fi
              fi
              
              exit 1
            fi
            
            echo "Waiting $RETRY_SLEEP seconds before next attempt..."
            sleep $RETRY_SLEEP
          done
          
          # Final message
          echo "Using EC2 instance $INSTANCE_ID with IP $EC2_IP for deployment"
      
      - name: Set up Kubernetes with retries
        run: |
          MAX_RETRIES=5
          
          # First ensure EC2 instance has SSH key access
          if [ -z "$EC2_IP" ]; then
            echo "ERROR: No EC2 instance IP found. Cannot continue with deployment."
            exit 1
          fi
          
          echo "Ensuring SSH key is authorized on the instance..."
          
          # Output the instance bootstrap logs to debug SSH issues
          echo "Checking instance console output for $INSTANCE_ID..."
          aws ec2 get-console-output --instance-id $INSTANCE_ID || echo "Unable to get console output"
          
          # Try with a script to setup Kubernetes
          for i in $(seq 1 $MAX_RETRIES); do
            echo "Attempt $i of $MAX_RETRIES to set up Kubernetes..."
            
            # Try via SSH
            echo "Testing SSH connection..."
            if ssh -v -o ConnectTimeout=30 -o StrictHostKeyChecking=no ${AWS_USER}@${EC2_IP} "echo Connection test successful"; then
              echo "SSH connection successful, setting up Kubernetes..."
              
              # Copy and execute setup scripts with detailed output
              echo "Copying setup scripts..."
              scp -v -o ConnectTimeout=30 -o StrictHostKeyChecking=no deploy/aws-k8s/k8s-setup.sh ${AWS_USER}@${EC2_IP}:~/ || { echo "SCP failed on attempt $i"; sleep 30; continue; }
              scp -v -o ConnectTimeout=30 -o StrictHostKeyChecking=no deploy/aws-k8s/k8s-deploy.sh ${AWS_USER}@${EC2_IP}:~/ || { echo "SCP failed on attempt $i"; sleep 30; continue; }
              
              echo "Setting scripts as executable..."
              ssh -o ConnectTimeout=30 -o StrictHostKeyChecking=no ${AWS_USER}@${EC2_IP} "chmod +x ~/k8s-setup.sh ~/k8s-deploy.sh" || { echo "SSH chmod failed on attempt $i"; sleep 30; continue; }
              
              echo "Running Kubernetes setup script..."
              ssh -o ConnectTimeout=240 -o StrictHostKeyChecking=no ${AWS_USER}@${EC2_IP} "export GROQ_API_KEY='${{ secrets.GROQ_API_KEY }}' && ~/k8s-setup.sh" || { echo "Kubernetes setup failed on attempt $i"; sleep 30; continue; }
              
              echo "Kubernetes setup completed on attempt $i"
              break
            else
              echo "SSH connection failed on attempt $i"
              
              if [ $i -eq $MAX_RETRIES ]; then
                echo "ERROR: Unable to connect to EC2 instance via SSH after $MAX_RETRIES attempts."
                # Continue with deployment as a best effort
                echo "Will attempt to continue with deployment as a best effort."
              else
                echo "Waiting 30 seconds before retrying..."
                sleep 30
              fi
            fi
          done
      
      - name: Get load balancer DNS
        run: |
          echo "Getting load balancer DNS..."
          LB_DNS=$(aws elbv2 describe-load-balancers \
            --names ${APP_NAME}-lb \
            --query 'LoadBalancers[0].DNSName' \
            --output text || echo "")
          
          if [ -z "$LB_DNS" ] || [ "$LB_DNS" == "None" ]; then
            echo "WARNING: Could not get load balancer DNS. Using placeholder for deployment."
            LB_DNS="pending.elb.amazonaws.com"
          fi
          
          echo "LB_DNS=$LB_DNS" >> $GITHUB_ENV
          echo "Load balancer DNS: $LB_DNS"
      
      - name: Deploy application
        run: |
          echo "Deploying application to Kubernetes..."
          
          # Test SSH connection again before proceeding
          echo "Testing SSH connection before deployment..."
          if ssh -v -o ConnectTimeout=30 -o StrictHostKeyChecking=no ${AWS_USER}@${EC2_IP} "echo Deployment preparation"; then
            echo "SSH connection successful, proceeding with deployment..."
            
            # Retry SCP if it fails
            MAX_RETRIES=3
            for i in $(seq 1 $MAX_RETRIES); do
              echo "Copying deployment script (attempt $i of $MAX_RETRIES)..."
              scp -v -o ConnectTimeout=30 -o StrictHostKeyChecking=no deploy/aws-k8s/k8s-deploy.sh ${AWS_USER}@${EC2_IP}:~/ && break
              echo "SCP failed, retrying in 10 seconds..."
              sleep 10
              
              if [ $i -eq $MAX_RETRIES ]; then
                echo "Warning: Failed to copy deployment script after $MAX_RETRIES attempts. Trying an alternative approach."
              fi
            done
            
            # Create a deployment script that can be fed to SSH directly as stdin
            echo "Creating inline deployment script..."
            cat > deploy-inline.sh << 'EOF'
            #!/bin/bash
            
            # Kubernetes deployment logic
            NAMESPACE="cold-email"
            
            # Create namespace if it doesn't exist
            kubectl get namespace $NAMESPACE || kubectl create namespace $NAMESPACE
            
            # Create deployment YAML
            cat > deployment.yaml << EOFINNER
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: cold-email
              namespace: cold-email
            spec:
              replicas: 1
              selector:
                matchLabels:
                  app: cold-email
              template:
                metadata:
                  labels:
                    app: cold-email
                spec:
                  containers:
                  - name: cold-email
                    image: $DOCKER_IMAGE
                    ports:
                    - containerPort: 8501
                    env:
                    - name: GROQ_API_KEY
                      value: "$GROQ_API_KEY"
            ---
            apiVersion: v1
            kind: Service
            metadata:
              name: cold-email
              namespace: cold-email
            spec:
              selector:
                app: cold-email
              ports:
              - port: 8501
                targetPort: 8501
              type: NodePort
            EOFINNER
            
            # Apply the deployment
            kubectl apply -f deployment.yaml
            
            # Get pod status
            kubectl get pods -n $NAMESPACE
            EOF
            
            # Make the script executable
            chmod +x deploy-inline.sh
            
            # Try deploying (multiple approaches)
            echo "Deploying application (attempt 1)..."
            if ssh -o ConnectTimeout=60 -o StrictHostKeyChecking=no ${AWS_USER}@${EC2_IP} "export DOCKER_IMAGE=${DOCKER_IMAGE} && export LB_DNS=${LB_DNS} && export GROQ_API_KEY='${{ secrets.GROQ_API_KEY }}' && bash -s" < deploy-inline.sh; then
              echo "Application deployed successfully!"
            else
              echo "First deployment attempt failed, trying alternative method..."
              
              # Try another approach
              echo "Deploying application (attempt 2)..."
              scp -o ConnectTimeout=30 -o StrictHostKeyChecking=no deploy-inline.sh ${AWS_USER}@${EC2_IP}:~/deploy-inline.sh || echo "SCP failed"
              ssh -o ConnectTimeout=60 -o StrictHostKeyChecking=no ${AWS_USER}@${EC2_IP} "chmod +x ~/deploy-inline.sh && export DOCKER_IMAGE=${DOCKER_IMAGE} && export LB_DNS=${LB_DNS} && export GROQ_API_KEY='${{ secrets.GROQ_API_KEY }}' && ~/deploy-inline.sh" || echo "SSH deployment failed, app may be partially deployed"
            fi
          else
            echo "ERROR: SSH connection failed. Cannot deploy application normally."
            echo "Application not deployed. Infrastructure exists, but application deployment failed."
          fi
      
      - name: Monitor deployment
        run: |
          echo "Checking Kubernetes deployment status..."
          
          # Give the deployment time to create resources
          echo "Waiting 30 seconds for resources to be created..."
          sleep 30
          
          # Try to get deployment status
          if ssh -o ConnectTimeout=20 -o StrictHostKeyChecking=no ${AWS_USER}@${EC2_IP} "kubectl get pods -n ${APP_NAME}"; then
            echo "Successfully retrieved pods"
            ssh -o ConnectTimeout=20 -o StrictHostKeyChecking=no ${AWS_USER}@${EC2_IP} "kubectl get svc -n ${APP_NAME}" || echo "Failed to get services"
          else
            echo "Failed to get deployment status via SSH"
          fi
          
          echo "Waiting 90 seconds for application to be fully available..."
          sleep 90
          
          # Check application health
          if [ ! -z "$LB_DNS" ] && [ "$LB_DNS" != "None" ] && [ "$LB_DNS" != "pending.elb.amazonaws.com" ]; then
            echo "Checking application health at http://${LB_DNS}/_stcore/health"
            curl -s -f -m 30 "http://${LB_DNS}/_stcore/health" || echo "Health check failed, application may need more time to become available"
            echo "Application should be accessible at: http://${LB_DNS}"
          else
            echo "Could not determine application URL. Check AWS console for Load Balancer details."
          fi
      
      - name: Verify EC2 network connectivity
        run: |
          echo "Verifying network connectivity to EC2 instances..."
          
          # Get instances
          INSTANCE_IDS=$(aws autoscaling describe-auto-scaling-groups \
            --auto-scaling-group-names ${APP_NAME}-asg \
            --query "AutoScalingGroups[0].Instances[*].InstanceId" \
            --output text)
          
          if [ -z "$INSTANCE_IDS" ]; then
            echo "No instances found in the auto scaling group."
            exit 1
          fi
          
          # Check each instance's network configuration
          for INSTANCE_ID in $INSTANCE_IDS; do
            echo "Checking network for instance $INSTANCE_ID..."
            
            # Get instance details
            INSTANCE_INFO=$(aws ec2 describe-instances --instance-ids $INSTANCE_ID)
            
            # Get subnet ID
            SUBNET_ID=$(echo "$INSTANCE_INFO" | jq -r '.Reservations[0].Instances[0].SubnetId')
            echo "Subnet ID: $SUBNET_ID"
            
            # Get VPC ID
            VPC_ID=$(echo "$INSTANCE_INFO" | jq -r '.Reservations[0].Instances[0].VpcId')
            echo "VPC ID: $VPC_ID"
            
            # Get public IP
            PUBLIC_IP=$(echo "$INSTANCE_INFO" | jq -r '.Reservations[0].Instances[0].PublicIpAddress')
            if [ "$PUBLIC_IP" == "null" ] || [ -z "$PUBLIC_IP" ]; then
              echo "WARNING: No public IP found for instance $INSTANCE_ID"
              continue
            fi
            echo "Public IP: $PUBLIC_IP"
            
            # Get security groups
            SG_IDS=$(echo "$INSTANCE_INFO" | jq -r '.Reservations[0].Instances[0].SecurityGroups[].GroupId')
            echo "Security Groups: $SG_IDS"
            
            # Check Network ACLs for the subnet
            echo "Checking Network ACLs for subnet $SUBNET_ID..."
            NACL_ASSOC=$(aws ec2 describe-network-acls \
              --filters "Name=association.subnet-id,Values=$SUBNET_ID" \
              --query "NetworkAcls[0].Associations[?SubnetId=='$SUBNET_ID'].NetworkAclId" \
              --output text)
            
            if [ -z "$NACL_ASSOC" ] || [ "$NACL_ASSOC" == "None" ]; then
              echo "No specific NACL found for subnet $SUBNET_ID, using default NACL"
              
              # Get default NACL for the VPC
              NACL_ID=$(aws ec2 describe-network-acls \
                --filters "Name=vpc-id,Values=$VPC_ID" "Name=default,Values=true" \
                --query "NetworkAcls[0].NetworkAclId" \
                --output text)
            else
              NACL_ID=$NACL_ASSOC
            fi
            
            echo "Network ACL ID: $NACL_ID"
            
            # Check if NACL allows SSH inbound
            echo "Checking if NACL allows SSH inbound traffic..."
            aws ec2 describe-network-acls \
              --network-acl-ids $NACL_ID \
              --query "NetworkAcls[0].Entries[?PortRange.From==22 && PortRange.To==22 && Egress==false]" \
              --output json
            
            # Check route table for the subnet
            echo "Checking route table for subnet $SUBNET_ID..."
            ROUTE_TABLE_ID=$(aws ec2 describe-route-tables \
              --filters "Name=association.subnet-id,Values=$SUBNET_ID" \
              --query "RouteTables[0].RouteTableId" \
              --output text)
            
            if [ -z "$ROUTE_TABLE_ID" ] || [ "$ROUTE_TABLE_ID" == "None" ]; then
              echo "No specific route table found for subnet $SUBNET_ID, using main route table"
              
              # Get main route table for the VPC
              ROUTE_TABLE_ID=$(aws ec2 describe-route-tables \
                --filters "Name=vpc-id,Values=$VPC_ID" "Name=association.main,Values=true" \
                --query "RouteTables[0].RouteTableId" \
                --output text)
            fi
            
            echo "Route table ID: $ROUTE_TABLE_ID"
            
            # Check for internet gateway route
            echo "Checking for internet gateway route..."
            aws ec2 describe-route-tables \
              --route-table-ids $ROUTE_TABLE_ID \
              --query "RouteTables[0].Routes[?DestinationCidrBlock=='0.0.0.0/0']" \
              --output json
            
            # Try to ping the instance
            echo "Trying to ping $PUBLIC_IP..."
            if ping -c 3 -W 5 $PUBLIC_IP; then
              echo "Successfully pinged $PUBLIC_IP"
            else
              echo "Could not ping $PUBLIC_IP, but this might be due to ICMP being blocked"
            fi
            
            # Try to check port 22 with netcat
            echo "Checking if port 22 is reachable on $PUBLIC_IP..."
            if nc -zv -w 5 $PUBLIC_IP 22; then
              echo "Port 22 is reachable on $PUBLIC_IP"
            else
              echo "Port 22 is not reachable on $PUBLIC_IP"
            fi
            
            echo "Network verification completed for instance $INSTANCE_ID"
            echo "------------------------------------------------------"
          done
      
      - name: Alternative deployment with SSM if SSH fails
        if: ${{ failure() || success() }}
        run: |
          echo "Attempting alternative deployment using AWS Systems Manager..."
          
          # Check if we have an instance ID
          if [ -z "$INSTANCE_ID" ]; then
            echo "No instance ID found, attempting to find a running instance..."
            INSTANCE_ID=$(aws ec2 describe-instances \
              --filters "Name=tag:aws:autoscaling:groupName,Values=${APP_NAME}-asg" "Name=instance-state-name,Values=running" \
              --query "Reservations[0].Instances[0].InstanceId" \
              --output text)
            
            if [ -z "$INSTANCE_ID" ] || [ "$INSTANCE_ID" == "None" ]; then
              echo "No running instances found. Checking if ASG exists..."
              
              ASG_EXISTS=$(aws autoscaling describe-auto-scaling-groups \
                --auto-scaling-group-names ${APP_NAME}-asg \
                --query "length(AutoScalingGroups)" \
                --output text 2>/dev/null || echo "0")
              
              if [ "$ASG_EXISTS" == "0" ]; then
                echo "No Auto Scaling Group found. Creating infrastructure from scratch..."
                chmod +x deploy/aws-k8s/aws-infrastructure.sh
                cd deploy/aws-k8s
                ./aws-infrastructure.sh
                cd ../..
                echo "Waiting for instances to be created and initialized (5 minutes)..."
                sleep 300
                
                # Try to get the instance ID again
                INSTANCE_ID=$(aws ec2 describe-instances \
                  --filters "Name=tag:aws:autoscaling:groupName,Values=${APP_NAME}-asg" "Name=instance-state-name,Values=running" \
                  --query "Reservations[0].Instances[0].InstanceId" \
                  --output text)
                
                if [ -z "$INSTANCE_ID" ] || [ "$INSTANCE_ID" == "None" ]; then
                  echo "Still no instances found after creating infrastructure. Trying to increase capacity..."
                  
                  # Try to increase ASG capacity
                  aws autoscaling set-desired-capacity \
                    --auto-scaling-group-name ${APP_NAME}-asg \
                    --desired-capacity 2
                  
                  echo "Waiting for additional instances (3 minutes)..."
                  sleep 180
                  
                  # Try one more time
                  INSTANCE_ID=$(aws ec2 describe-instances \
                    --filters "Name=tag:aws:autoscaling:groupName,Values=${APP_NAME}-asg" "Name=instance-state-name,Values=running" \
                    --query "Reservations[0].Instances[0].InstanceId" \
                    --output text)
                fi
              else
                echo "Auto Scaling Group exists but no running instances found. Checking desired capacity..."
                
                DESIRED_CAPACITY=$(aws autoscaling describe-auto-scaling-groups \
                  --auto-scaling-group-names ${APP_NAME}-asg \
                  --query "AutoScalingGroups[0].DesiredCapacity" \
                  --output text)
                
                echo "Current desired capacity: $DESIRED_CAPACITY"
                
                if [ "$DESIRED_CAPACITY" == "0" ]; then
                  echo "Increasing desired capacity to 1..."
                  aws autoscaling set-desired-capacity \
                    --auto-scaling-group-name ${APP_NAME}-asg \
                    --desired-capacity 1
                else
                  echo "Terminating any unhealthy instances and refreshing ASG..."
                  aws autoscaling start-instance-refresh \
                    --auto-scaling-group-name ${APP_NAME}-asg \
                    --strategy Rolling \
                    --preferences "MinHealthyPercentage=0"
                fi
                
                echo "Waiting for new instances to launch (5 minutes)..."
                sleep 300
                
                # Try to get the instance ID again
                INSTANCE_ID=$(aws ec2 describe-instances \
                  --filters "Name=tag:aws:autoscaling:groupName,Values=${APP_NAME}-asg" "Name=instance-state-name,Values=running" \
                  --query "Reservations[0].Instances[0].InstanceId" \
                  --output text)
              fi
              
              if [ -z "$INSTANCE_ID" ] || [ "$INSTANCE_ID" == "None" ]; then
                echo "Still no running instances found after recovery attempts."
                echo "Please check AWS console for errors in instance launch."
                
                # Debug information
                echo "ASG Status:"
                aws autoscaling describe-auto-scaling-groups --auto-scaling-group-names ${APP_NAME}-asg || echo "ASG not found"
                
                echo "Launch Template:"
                aws ec2 describe-launch-templates --launch-template-names ${APP_NAME}-launch-template || echo "Launch template not found"
                
                echo "Security Groups:"
                aws ec2 describe-security-groups --group-names ${APP_NAME}-ec2-sg || echo "Security group not found"
                
                exit 1
              fi
            fi
            
            echo "Using instance $INSTANCE_ID for SSM deployment."
          fi
          
          # Check if SSM Agent is installed and running
          echo "Checking if SSM Agent is installed on instance $INSTANCE_ID..."
          SSM_STATUS=$(aws ssm describe-instance-information \
            --filters "Key=InstanceIds,Values=$INSTANCE_ID" \
            --query "InstanceInformationList[0].PingStatus" \
            --output text 2>/dev/null || echo "UNKNOWN")
          
          if [ "$SSM_STATUS" == "Online" ]; then
            echo "SSM Agent is online on instance $INSTANCE_ID"
          else
            echo "SSM Agent is not online or not installed on instance $INSTANCE_ID"
            echo "Attempting to install and configure SSM Agent via user data update..."
            
            # Terminate and replace the instance to get a new one with SSM
            echo "Replacing instance to get one with SSM Agent..."
            aws autoscaling terminate-instance-in-auto-scaling-group \
              --instance-id $INSTANCE_ID \
              --should-decrement-desired-capacity false
            
            echo "Waiting for new instance to become available..."
            sleep 300  # Wait 5 minutes for new instance to be created and initialized
            
            # Get the new instance ID
            INSTANCE_ID=$(aws ec2 describe-instances \
              --filters "Name=tag:aws:autoscaling:groupName,Values=${APP_NAME}-asg" "Name=instance-state-name,Values=running" \
              --query "Reservations[0].Instances[0].InstanceId" \
              --output text)
            
            if [ -z "$INSTANCE_ID" ] || [ "$INSTANCE_ID" == "None" ]; then
              echo "No new running instances found after replacement. Cannot continue with SSM deployment."
              exit 1
            fi
            
            echo "Using new instance $INSTANCE_ID for SSM deployment."
            
            # Check if SSM Agent is now online
            echo "Checking if SSM Agent is installed on new instance $INSTANCE_ID..."
            SSM_STATUS=$(aws ssm describe-instance-information \
              --filters "Key=InstanceIds,Values=$INSTANCE_ID" \
              --query "InstanceInformationList[0].PingStatus" \
              --output text 2>/dev/null || echo "UNKNOWN")
            
            if [ "$SSM_STATUS" != "Online" ]; then
              echo "SSM Agent is still not online on new instance $INSTANCE_ID. Cannot continue with SSM deployment."
              echo "Please check AWS console and ensure SSM Agent is installed on EC2 instances."
              exit 1
            fi
          fi
          
          # Try to deploy using SSM
          echo "Deploying application using SSM commands..."
          
          # Step 1: Install Docker and Kubernetes if not installed
          echo "Verifying Docker and Kubernetes installation..."
          INSTALL_COMMAND_ID=$(aws ssm send-command \
            --instance-ids $INSTANCE_ID \
            --document-name "AWS-RunShellScript" \
            --parameters '{
              "commands":[
                "#!/bin/bash",
                "# Install Docker if not present",
                "if ! command -v docker &>/dev/null; then",
                "  apt-get update",
                "  apt-get install -y apt-transport-https ca-certificates curl software-properties-common",
                "  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -",
                "  add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\"",
                "  apt-get update && apt-get install -y docker-ce docker-ce-cli containerd.io",
                "  systemctl enable docker && systemctl start docker",
                "fi",
                "# Install Kubernetes if not present",
                "if ! command -v kubectl &>/dev/null; then",
                "  curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -",
                "  echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" > /etc/apt/sources.list.d/kubernetes.list",
                "  apt-get update && apt-get install -y kubelet kubeadm kubectl",
                "  apt-mark hold kubelet kubeadm kubectl",
                "fi",
                "# Check Docker and Kubernetes versions",
                "docker version",
                "kubectl version --client || echo \"kubectl client only version check failed\""
              ]
            }' \
            --query "Command.CommandId" --output text)
          
          echo "Installation command sent with ID: $INSTALL_COMMAND_ID"
          echo "Waiting for installation command to complete..."
          
          aws ssm wait command-executed \
            --command-id $INSTALL_COMMAND_ID \
            --instance-id $INSTANCE_ID
          
          # Step 2: Initialize Kubernetes if not already initialized
          echo "Initializing Kubernetes..."
          INIT_COMMAND_ID=$(aws ssm send-command \
            --instance-ids $INSTANCE_ID \
            --document-name "AWS-RunShellScript" \
            --parameters '{
              "commands":[
                "#!/bin/bash",
                "if [ ! -f \"/etc/kubernetes/admin.conf\" ]; then",
                "  # Disable swap",
                "  swapoff -a",
                "  # Initialize Kubernetes",
                "  kubeadm init --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors=all || echo \"Kubernetes init failed but continuing\"",
                "  # Set up kubectl config",
                "  mkdir -p $HOME/.kube",
                "  cp -f /etc/kubernetes/admin.conf $HOME/.kube/config",
                "  chown $(id -u):$(id -g) $HOME/.kube/config",
                "  # Allow pods on master node",
                "  kubectl taint nodes --all node-role.kubernetes.io/control-plane- || true",
                "  # Install CNI",
                "  kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml || echo \"Flannel installation failed but continuing\"",
                "  echo \"Kubernetes initialized\"",
                "else",
                "  echo \"Kubernetes already initialized\"",
                "fi",
                "# Create namespace",
                "kubectl create namespace cold-email 2>/dev/null || echo \"Namespace already exists\""
              ]
            }' \
            --query "Command.CommandId" --output text)
          
          echo "Initialization command sent with ID: $INIT_COMMAND_ID"
          echo "Waiting for initialization command to complete..."
          
          aws ssm wait command-executed \
            --command-id $INIT_COMMAND_ID \
            --instance-id $INSTANCE_ID
          
          # Step 3: Deploy the application
          echo "Deploying application via SSM..."
          
          # Create deployment YAML via SSM
          DEPLOY_COMMAND_ID=$(aws ssm send-command \
            --instance-ids $INSTANCE_ID \
            --document-name "AWS-RunShellScript" \
            --parameters "{
              \"commands\":[
                \"#!/bin/bash\",
                \"# Create deployment YAML\",
                \"cat > /tmp/deployment.yaml << 'EOF'\",
                \"apiVersion: apps/v1\",
                \"kind: Deployment\",
                \"metadata:\",
                \"  name: cold-email\",
                \"  namespace: cold-email\",
                \"spec:\",
                \"  replicas: 1\",
                \"  selector:\",
                \"    matchLabels:\",
                \"      app: cold-email\",
                \"  template:\",
                \"    metadata:\",
                \"      labels:\",
                \"        app: cold-email\",
                \"    spec:\",
                \"      containers:\",
                \"      - name: cold-email\",
                \"        image: ${DOCKER_IMAGE}\",
                \"        ports:\",
                \"        - containerPort: 8501\",
                \"        env:\",
                \"        - name: GROQ_API_KEY\",
                \"          value: \\\"${{ secrets.GROQ_API_KEY }}\\\"\",
                \"EOF\",
                \"# Create service YAML\",
                \"cat > /tmp/service.yaml << 'EOF'\",
                \"apiVersion: v1\",
                \"kind: Service\",
                \"metadata:\",
                \"  name: cold-email\",
                \"  namespace: cold-email\",
                \"spec:\",
                \"  selector:\",
                \"    app: cold-email\",
                \"  ports:\",
                \"  - port: 8501\",
                \"    targetPort: 8501\",
                \"  type: NodePort\",
                \"EOF\",
                \"# Apply manifests\",
                \"kubectl apply -f /tmp/deployment.yaml\",
                \"kubectl apply -f /tmp/service.yaml\",
                \"# Display resources\",
                \"kubectl get pods,svc -n cold-email\"
              ]
            }" \
            --query "Command.CommandId" --output text)
          
          echo "Deployment command sent with ID: $DEPLOY_COMMAND_ID"
          echo "Waiting for deployment command to complete..."
          
          aws ssm wait command-executed \
            --command-id $DEPLOY_COMMAND_ID \
            --instance-id $INSTANCE_ID
          
          # Get output from the deployment command
          echo "Deployment command output:"
          aws ssm get-command-invocation \
            --command-id $DEPLOY_COMMAND_ID \
            --instance-id $INSTANCE_ID \
            --query "StandardOutputContent" \
            --output text
          
          # Check deployment status
          echo "Checking deployment status..."
          STATUS_COMMAND_ID=$(aws ssm send-command \
            --instance-ids $INSTANCE_ID \
            --document-name "AWS-RunShellScript" \
            --parameters '{
              "commands":[
                "kubectl get pods -n cold-email",
                "kubectl get svc -n cold-email",
                "NODE_IP=$(curl -s http://169.254.169.254/latest/meta-data/public-ipv4)",
                "NODE_PORT=$(kubectl get svc -n cold-email cold-email -o jsonpath=\'{.spec.ports[0].nodePort}\')",
                "echo \"Application should be accessible at: http://${NODE_IP}:${NODE_PORT}\""
              ]
            }' \
            --query "Command.CommandId" --output text)
          
          echo "Status command sent with ID: $STATUS_COMMAND_ID"
          aws ssm wait command-executed \
            --command-id $STATUS_COMMAND_ID \
            --instance-id $INSTANCE_ID
          
          echo "Status command output:"
          aws ssm get-command-invocation \
            --command-id $STATUS_COMMAND_ID \
            --instance-id $INSTANCE_ID \
            --query "StandardOutputContent" \
            --output text
          
          echo "SSM deployment completed!"
      
      - name: Debug ASG and launch issues
        if: ${{ failure() || success() }}
        run: |
          echo "Debugging Auto Scaling Group and Instance Launch issues..."
          
          # Check if ASG exists first
          ASG_EXISTS=$(aws autoscaling describe-auto-scaling-groups \
            --auto-scaling-group-names ${APP_NAME}-asg \
            --query "length(AutoScalingGroups)" \
            --output text 2>/dev/null || echo "0")
          
          if [ "$ASG_EXISTS" == "0" ]; then
            echo "Auto Scaling Group does not exist. Cannot proceed with debugging."
            exit 0
          fi
          
          # Get ASG details
          echo "Getting Auto Scaling Group details..."
          aws autoscaling describe-auto-scaling-groups \
            --auto-scaling-group-names ${APP_NAME}-asg
          
          # Get launch template details
          echo "Getting Launch Template details..."
          LT_NAME="${APP_NAME}-launch-template"
          aws ec2 describe-launch-templates \
            --launch-template-names $LT_NAME || echo "Launch template not found"
          
          # Get scaling activities (shows errors in instance launches)
          echo "Checking Auto Scaling Activities for errors..."
          aws autoscaling describe-scaling-activities \
            --auto-scaling-group-name ${APP_NAME}-asg \
            --max-items 10
          
          # List any existing instances, even if not running
          echo "Listing all instances in the ASG (any state)..."
          INSTANCE_IDS=$(aws autoscaling describe-auto-scaling-groups \
            --auto-scaling-group-names ${APP_NAME}-asg \
            --query "AutoScalingGroups[0].Instances[*].InstanceId" \
            --output text)
          
          if [ -z "$INSTANCE_IDS" ]; then
            echo "No instances found in the Auto Scaling Group at all."
          else
            echo "Found instances: $INSTANCE_IDS"
            
            # Check each instance's state and possible status reason
            for INSTANCE_ID in $INSTANCE_IDS; do
              echo "Details for instance $INSTANCE_ID:"
              aws ec2 describe-instances --instance-ids $INSTANCE_ID \
                --query "Reservations[0].Instances[0].{State:State.Name,StateReason:StateReason,LaunchTime:LaunchTime}"
              
              # If instance is in terminated state, get the reason
              STATE=$(aws ec2 describe-instances --instance-ids $INSTANCE_ID \
                --query "Reservations[0].Instances[0].State.Name" \
                --output text 2>/dev/null || echo "unknown")
              
              if [ "$STATE" == "terminated" ] || [ "$STATE" == "shutting-down" ]; then
                echo "Terminated instance reason:"
                aws ec2 describe-instances --instance-ids $INSTANCE_ID \
                  --query "Reservations[0].Instances[0].StateReason"
              fi
            done
          fi
          
          # Check if there were any problems with the IAM role
          echo "Checking IAM role..."
          aws iam get-role --role-name ${APP_NAME}-ssm-role || echo "IAM role not found"
          
          # Check security group
          echo "Checking security group..."
          SG_ID=$(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=${APP_NAME}-ec2-sg" \
            --query "SecurityGroups[0].GroupId" \
            --output text 2>/dev/null || echo "")
          
          if [ -z "$SG_ID" ] || [ "$SG_ID" == "None" ]; then
            echo "Security group not found"
          else
            echo "Security group found: $SG_ID"
            aws ec2 describe-security-groups --group-ids $SG_ID
          fi
          
          echo "Debug information complete." 